{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5b9bd7-db29-4535-959d-fb43560177d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的模块\n",
    "import csv\n",
    "import os\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 记录开始时间\n",
    "time1 = time.time()\n",
    "\n",
    "def connect_to_database():\n",
    "    \"\"\"连接到数据库\"\"\"\n",
    "    return pymysql.connect(host='172.16.50.150', user='root', password='fz@2023&', database='input')\n",
    "\n",
    "def execute_query(cursor, query):\n",
    "    \"\"\"执行数据库查询\"\"\"\n",
    "    cursor.execute(query)\n",
    "\n",
    "def insert_data(cursor, table_name, data_frame):\n",
    "    \"\"\"将数据插入数据库\"\"\"\n",
    "    existing_columns = get_table_columns(cursor, table_name)\n",
    "    valid_columns = [col for col in data_frame.columns if col in existing_columns]\n",
    "    valid_columns_not_null = [col for col in valid_columns if data_frame[col].notnull().any()]\n",
    "\n",
    "    insert_sql = f\"INSERT INTO {table_name} ({', '.join(valid_columns)}) VALUES ({', '.join(['%s'] * len(valid_columns))}) ON DUPLICATE KEY UPDATE {', '.join([f'`{col}` = VALUES(`{col}`)' for col in valid_columns_not_null])};\"\n",
    "    values = [tuple(str(row[col]) for col in valid_columns) for index, row in data_frame.iterrows()]\n",
    "\n",
    "    cursor.executemany(insert_sql, values)\n",
    "\n",
    "def get_table_columns(cursor, table_name):\n",
    "    \"\"\"获取数据库表的列\"\"\"\n",
    "    cursor.execute(f\"SHOW COLUMNS FROM {table_name}\")\n",
    "    return [column[0] for column in cursor.fetchall()]\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    # 记录开始时间\n",
    "    time_start = time.time()\n",
    "\n",
    "    # 连接到数据库\n",
    "    conn = connect_to_database()\n",
    "\n",
    "    # 查询配置表\n",
    "    with conn.cursor() as c1:\n",
    "        execute_query(c1, 'SELECT * FROM Oech_Analyse_J_configure')\n",
    "        columns = [column[0] for column in c1.description]\n",
    "        df_config = pd.DataFrame(c1.fetchall(), columns=columns)\n",
    "\n",
    "    # 查询电厂文档表\n",
    "    with conn.cursor() as c2:\n",
    "        execute_query(c2, 'SELECT Power_plant_name, Unit_number, num_type, date, file_name FROM Oech_Analyse_power_plant_j_documentation WHERE is_write != \"是\" OR is_write IS NULL')\n",
    "        table_data1 = c2.fetchall()\n",
    "\n",
    "    # 构造文件路径列表\n",
    "    full_paths = [os.path.join('Z:/power_plant_data_j', *row[:-1], row[-1]).replace('\\\\', '/') for row in table_data1]\n",
    "    table_data2 = [row + (path,) for row, path in zip(table_data1, full_paths)]\n",
    "\n",
    "    # 构建列名替换字典\n",
    "    header_replacement_dict = {value: column for column, values in df_config.items() for value in values}\n",
    "\n",
    "    with conn.cursor() as cursor:\n",
    "        # 遍历文件数据列表\n",
    "        for a in tqdm(table_data2, desc='处理文件', unit='文件'):\n",
    "            try:\n",
    "                with open(a[-1], 'r', encoding='utf-8') as csvfile:\n",
    "                    reader = csv.reader(csvfile)\n",
    "                    rows = [row for index, row in enumerate(reader) if index not in [0, 2]]\n",
    "\n",
    "                columns2 = rows[0]\n",
    "                data_rows = [row[:-1] for row in rows[1:]]\n",
    "                new_data = pd.DataFrame(data_rows, columns=columns2)\n",
    "\n",
    "                start_time = new_data.iloc[:, 0].iloc[0]\n",
    "                end_time = new_data.iloc[:, 0].iloc[-1]\n",
    "                second_time = new_data.iloc[:, 0].iloc[1]\n",
    "\n",
    "                start_time = datetime.datetime.strptime(start_time.strip(), \"%Y/%m/%d %H:%M:%S\")\n",
    "                second_time = datetime.datetime.strptime(second_time.strip(), \"%Y/%m/%d %H:%M:%S\")\n",
    "                end_time = datetime.datetime.strptime(end_time.strip(), \"%Y/%m/%d %H:%M:%S\")\n",
    "\n",
    "                data_granularity = second_time - start_time\n",
    "                data_granularity = format(data_granularity)[-10:]\n",
    "\n",
    "                new_data.columns = [header_replacement_dict[col] if col in header_replacement_dict else col for col in new_data.columns]\n",
    "\n",
    "                new_data['power_plant_id'] = a[0][:3]\n",
    "                new_data['power_plant_name'] = a[0]\n",
    "                new_data['unit_number'] = a[1]\n",
    "                new_data['num_type'] = a[2]\n",
    "                new_data['data_granularity'] = data_granularity\n",
    "\n",
    "                valid_data = new_data.loc[:, ~new_data.columns.duplicated()]\n",
    "\n",
    "                for table_name in ['Oech_Analyse_J_VIB', 'Oech_Analyse_J_TCA_FGH', 'Oech_Analyse_J_Operational_history', 'Oech_Analyse_J_LUBE_OIL', 'Oech_Analyse_J_FUEL_GAS', 'Oech_Analyse_J_ECA', 'Oech_Analyse_J_DTC', 'Oech_Analyse_J_CPFM', 'Oech_Analyse_J_CONTROL', 'Oech_Analyse_J_BPT', 'Oech_Analyse_J_BEARING', 'Oech_Analyse_J_AIR']:\n",
    "                    insert_data(cursor, table_name, valid_data)\n",
    "\n",
    "                update_sql = f\"UPDATE Oech_Analyse_power_plant_j_documentation SET is_write = '是' WHERE Power_plant_name = '{a[0]}' AND Unit_number = '{a[1]}' AND file_name = '{a[4]}';\"\n",
    "                cursor.execute(update_sql)\n",
    "                conn.commit()\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                tqdm.write(f\"文件未找到: {a[-1]} 跳过下一个文件。\")\n",
    "                continue\n",
    "\n",
    "            except Exception as e:\n",
    "                tqdm.write(f\"处理文件时出错 {a[-1]}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # 创建数据库连接\n",
    "    connection = connect_to_database()\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # 构造插入语句\n",
    "    insert_query = f\"\"\"\n",
    "    INSERT INTO Oech_Analyse_data_j_granularity ( date_part, power_plant_id, unit_number, num_type, min_time_part, max_time_part, data_granularity ) \n",
    "    SELECT\n",
    "    date_part,\n",
    "    power_plant_id,\n",
    "    unit_number,\n",
    "    num_type,\n",
    "    min_time_part,\n",
    "    max_time_part,\n",
    "    data_granularity \n",
    "    FROM\n",
    "        (\n",
    "            WITH RankedData AS (\n",
    "            SELECT DISTINCT DATE\n",
    "                ( TIME ) AS date_part,\n",
    "                TIME ( TIME ) AS time_part,\n",
    "                power_plant_id,\n",
    "                unit_number,\n",
    "                num_type,\n",
    "                data_granularity,\n",
    "                ROW_NUMBER() OVER (\n",
    "                    PARTITION BY DATE ( TIME ),\n",
    "                    power_plant_id,\n",
    "                    unit_number,\n",
    "                    num_type,\n",
    "                    data_granularity \n",
    "                ORDER BY\n",
    "                TIME ( TIME )) AS rn_asc,\n",
    "                ROW_NUMBER() OVER ( PARTITION BY DATE ( TIME ), power_plant_id, unit_number, num_type, data_granularity ORDER BY TIME ( TIME ) DESC ) \n",
    "                AS rn_desc \n",
    "            FROM\n",
    "                Oech_Analyse_J_Operational_history \n",
    "            ) SELECT\n",
    "            power_plant_id,\n",
    "            unit_number,\n",
    "            num_type,\n",
    "            date_part,\n",
    "            MIN( CASE WHEN rn_asc = 1 THEN time_part END ) AS min_time_part,\n",
    "            MAX( CASE WHEN rn_desc = 1 THEN time_part END ) AS max_time_part,\n",
    "            data_granularity \n",
    "        FROM\n",
    "            RankedData \n",
    "        GROUP BY\n",
    "            date_part,\n",
    "            power_plant_id,\n",
    "            unit_number,\n",
    "            num_type,\n",
    "            data_granularity \n",
    "        ) AS Result \n",
    "    ON DUPLICATE KEY UPDATE min_time_part = Result.min_time_part,\n",
    "    max_time_part = Result.max_time_part,\n",
    "    data_granularity = Result.data_granularity;\n",
    "    \"\"\"\n",
    "\n",
    "    cursor.execute(insert_query)\n",
    "\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "    # 记录结束时间\n",
    "    time_end = time.time()\n",
    "    time_sum = time_end - time_start\n",
    "    print(f\"执行时间: {time_sum} 秒\")\n",
    "\n",
    "# 主程序入口\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# 记录结束时间\n",
    "time2 = time.time()\n",
    "time_sum = time2 - time1\n",
    "print(f\"执行时间: {time_sum} 秒\")\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "# 关闭cmd窗口\n",
    "os.system(\"taskkill /f /im cmd.exe\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
